@inproceedings{ayalaReducingHallucinationStructured2024,
  title = {Reducing Hallucination in Structured Outputs via {{Retrieval-Augmented Generation}}},
  booktitle = {Proceedings of the 2024 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 6: {{Industry Track}})},
  author = {Ayala, Orlando and Bechard, Patrice},
  editor = {Yang, Yi and Davani, Aida and Sil, Avi and Kumar, Anoop},
  date = {2024-06},
  pages = {228--238},
  publisher = {Association for Computational Linguistics},
  location = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.naacl-industry.19},
  url = {https://aclanthology.org/2024.naacl-industry.19/},
  urldate = {2025-09-30},
  abstract = {A current limitation of Generative AI (GenAI) is its propensity to hallucinate. While Large Language Models (LLM) have taken the world by storm, without eliminating or at least reducing hallucination, real-world GenAI systems will likely continue to face challenges in user adoption. In the process of deploying an enterprise application that produces workflows from natural language requirements, we devised a system leveraging Retrieval-Augmented Generation (RAG) to improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucination and allows the generalization of our LLM to out-of-domain settings. In addition, we show that using a small, well-trained retriever can reduce the size of the accompanying LLM at no loss in performance, thereby making deployments of LLM-based systems less resource-intensive.},
  eventtitle = {{{NAACL-HLT}} 2024},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/zepi/Zotero/storage/DU492KSD/Ayala and Bechard - 2024 - Reducing hallucination in structured outputs via Retrieval-Augmented Generation.pdf;/home/zepi/Zotero/storage/UNLIYJX6/Béchard und Ayala - 2024 - Reducing hallucination in structured outputs via Retrieval-Augmented Generation.pdf;/home/zepi/Zotero/storage/YSQFD5GG/2404.html}
}

@article{chenEntityRecognitionMethod2024,
  title = {Entity {{Recognition Method}} for {{Key Information}} of {{Police Records Based}} on {{Bert-Bilstm-Selfatt-Crf}}},
  author = {Chen, Xiaoang and Tong, Xin and Lin, Hailun and Xing, Yunfei},
  date = {2024},
  journaltitle = {Academic Journal of Computing \& Information Science},
  shortjournal = {AJCIS},
  volume = {7},
  number = {1},
  issn = {26165775},
  doi = {10.25236/AJCIS.2024.070112},
  url = {https://francis-press.com/papers/14562},
  urldate = {2025-09-30},
  file = {/home/zepi/Zotero/storage/9FJN34P6/2024 - Entity Recognition Method for Key Information of Police Records Based on Bert-Bilstm-Selfatt-Crf.pdf}
}

@online{DecodingStrategiesLarge2025,
  title = {Decoding {{Strategies}} in {{Large Language Models}}},
  date = {2025-09-07},
  url = {https://huggingface.co/blog/mlabonne/decoding-strategies},
  urldate = {2025-10-04},
  abstract = {A Blog post by Maxime Labonne on Hugging Face},
  file = {/home/zepi/Zotero/storage/2BF2CQWW/decoding-strategies.html}
}

@article{geroSelfVerificationImprovesFewShot2023,
  title = {Self-{{Verification Improves Few-Shot Clinical Information Extraction}}},
  author = {Gero, Zelalem and Singh, Chandan and Cheng, Hao and Naumann, Tristan and Galley, Michel and Gao, Jianfeng and Poon, Hoifung},
  date = {2023},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.00024},
  url = {https://arxiv.org/abs/2306.00024},
  urldate = {2025-09-30},
  abstract = {Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts.},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/zepi/Zotero/storage/NS595T4P/Gero et al. - 2023 - Self-Verification Improves Few-Shot Clinical Information Extraction.pdf}
}

@article{haasTechnischeIntegrationGrosser2025,
  title = {Technische Integration großer Sprachmodelle in Unternehmen: Entwicklung eines Frameworks zur technischen Vorbereitung von Unternehmen auf die Nutzung von Sprachmodellen},
  shorttitle = {Technische Integration großer Sprachmodelle in Unternehmen},
  author = {Haas, Christopher},
  date = {2025},
  pages = {1494 KB, vi, 92 pages},
  publisher = {FH CAMPUS 02 (CAMPUS 02 Fachhochschule der Wirtschaft)},
  doi = {10.58023/1155},
  url = {https://opus.campus02.at/1155},
  urldate = {2025-09-16},
  abstract = {The increasing adoption of Large Language Models (LLMs) in corporate environments offers new opportunities for automating knowledge retrieval, content generation, and decision support. At the sametime, their implementation imposes significant technical requirements on existing IT infrastructures and data architectures. This study develops a structured framework to assist companies in the technical preparation for LLM integration.The research focuses on the key technical requirements that organizations must meet to effective-lyintegrate LLMs into specific tasks within their business processes. A particular emphasis is placed onanalyzing internal corporate data formats and structures, as well as the necessary preprocessing steps to ensure LLM compatibility. Additionally, best practices for data archiving and provisioning are identified.Furthermore, a comparative analysis of on-premises and cloud-based hosting solutions, as well as selfhosted and externally hosted LLMs, is conducted, with scalability and security serving as the primaryevaluation criteria. Another focal point of the study is Retrieval-Augmented Generation (RAG) as amethod to enhance LLM performance through the utilization of company-specific knowledge bases.This study follows the Design Science Research methodology and combines an extensive literaturereview with semi-structured expert interviews, which primarily serve to evaluate and refine the developed framework. The evaluation assesses its applicability across different industries and its potential to support IT departments and decision-makers in the strategic implementation of LLMs.The findings of this study aim to provide practical recommendations for the technical implementa-tionof LLMs in businesses. They address both infrastructural and data-specific requirements and offer awell-founded decision-making basis for the introduction and utilization of LLMs.},
  langid = {ngerman},
  keywords = {658 Allgemeines Management},
  file = {/home/zepi/Zotero/storage/TLY3KXPV/Haas - 2025 - Technische Integration großer Sprachmodelle in Unternehmen  Entwicklung eines Frameworks zur techni.pdf}
}

@online{IntroductionLargeLanguage,
  title = {Introduction to {{Large Language Models}} | {{Machine Learning}}},
  url = {https://developers.google.com/machine-learning/resources/intro-llms},
  urldate = {2025-10-02},
  langid = {english},
  organization = {Google for Developers},
  file = {/home/zepi/Zotero/storage/P9SIWTTC/intro-llms.html}
}

@online{kelbertWieFunktionierenLLMs2024,
  title = {Wie funktionieren LLMs? Ein Blick ins Innere großer Sprachmodelle - Blog des Fraunhofer IESE},
  shorttitle = {Wie funktionieren LLMs?},
  author = {Kelbert, Patricia, Dr Julien Siebert},
  date = {2024-06-17T07:15:06+00:00},
  url = {https://www.iese.fraunhofer.de/blog/wie-funktionieren-llms/},
  urldate = {2025-10-02},
  abstract = {Woraus bestehen große Sprachmodelle (Large Language Models)? KI-Experten und Expertinnen erklären die Funktionsweise von LLMs.},
  langid = {ngerman},
  organization = {Fraunhofer IESE},
  file = {/home/zepi/Zotero/storage/EAXYFDSU/wie-funktionieren-llms.html}
}

@inproceedings{kuCrimeInformationExtraction2008,
  title = {Crime {{Information Extraction}} from {{Police}} and {{Witness Narrative Reports}}},
  booktitle = {2008 {{IEEE Conference}} on {{Technologies}} for {{Homeland Security}}},
  author = {Ku, Chih Hao and Iriberri, Alicia and Leroy, Gondy},
  date = {2008-05},
  pages = {193--198},
  publisher = {IEEE},
  location = {Waltham, MA, USA},
  doi = {10.1109/THS.2008.4534448},
  url = {http://ieeexplore.ieee.org/document/4534448/},
  urldate = {2025-09-29},
  eventtitle = {2008 {{IEEE Conference}} on {{Technologies}} for {{Homeland Security}}},
  isbn = {978-1-4244-1977-7},
  file = {/home/zepi/Zotero/storage/PPXZHYIE/Ku et al. - 2008 - Crime Information Extraction from Police and Witness Narrative Reports.pdf}
}

@inproceedings{kuNaturalLanguageProcessing2008,
  title = {Natural Language Processing and E-{{Government}}: Crime Information Extraction from Heterogeneous Data Sources},
  shorttitle = {Natural Language Processing and E-{{Government}}},
  booktitle = {Proceedings of the 2008 International Conference on {{Digital}} Government Research},
  author = {Ku, Chih Hao and Iriberri, Alicia and Leroy, Gondy},
  date = {2008-05-18},
  series = {Dg.o '08},
  pages = {162--170},
  publisher = {Digital Government Society of North America},
  location = {Montreal, Canada},
  abstract = {Much information that could help solve and prevent crimes is never gathered because the reporting methods available to citizens and law enforcement personnel are not optimal. Detectives do not have sufficient time to interview crime victims and witnesses. Moreover, many victims and witnesses are too scared or embarrassed to report incidents. We are developing an interviewing system that will help collect such information. We report here on one component, the crime information extraction module, which uses natural language processing to extract crime information from police reports, newspaper articles, and victims' and witnesses' crime narratives. We tested our approach with two types of document: police and witness narrative reports. Our algorithms extract crime-related information, namely weapons, vehicles, time, people, clothes, and locations. We achieved high precision (96\%) and recall (83\%) for police narrative reports and comparable precision (93\%) but somewhat lower recall (77\%) for witness narrative reports. The difference in recall was significant at p \&lt; .05. We then used a spell checker to evaluate if this would help with witness narrative processing. We found that both precision (94\%) and recall (79\%) improved slightly.},
  isbn = {978-1-60558-099-9}
}

@online{liAutoFlowAutomatedWorkflow2024,
  title = {{{AutoFlow}}: {{Automated Workflow Generation}} for {{Large Language Model Agents}} [{{Preprint}}]},
  shorttitle = {{{AutoFlow}}},
  author = {Li, Zelong and Xu, Shuyuan and Mei, Kai and Hua, Wenyue and Rama, Balaji and Raheja, Om and Wang, Hao and Zhu, He and Zhang, Yongfeng},
  date = {2024},
  doi = {10.48550/ARXIV.2407.12821},
  url = {https://arxiv.org/abs/2407.12821},
  urldate = {2025-09-30},
  abstract = {Recent advancements in Large Language Models (LLMs) have shown significant progress in understanding complex natural language. One important application of LLM is LLM-based AI Agent, which leverages the ability of LLM as well as external tools for complex-task solving. To make sure LLM Agents follow an effective and reliable procedure to solve the given task, manually designed workflows are usually used to guide the working mechanism of agents. However, manually designing the workflows requires considerable efforts and domain knowledge, making it difficult to develop and deploy agents on massive scales. To address these issues, we propose AutoFlow, a framework designed to automatically generate workflows for agents to solve complex tasks. AutoFlow takes natural language program as the format of agent workflow and employs a workflow optimization procedure to iteratively optimize the workflow quality. Besides, this work offers two workflow generation methods: fine-tuning-based and in-context-based methods, making the AutoFlow framework applicable to both open-source and closed-source LLMs. Experimental results show that our framework can produce robust and reliable agent workflows. We believe that the automatic generation and interpretation of workflows in natural language represent a promising paradigm for solving complex tasks, particularly with the rapid development of LLMs. The source code of this work is available at https://github.com/agiresearch/AutoFlow.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/zepi/Zotero/storage/U8BQC42J/Li et al. - 2024 - AutoFlow Automated Workflow Generation for Large Language Model Agents [Preprint].pdf}
}

@article{liBiomedRAGRetrievalAugmented2025,
  title = {{{BiomedRAG}}: {{A}} Retrieval Augmented Large Language Model for Biomedicine},
  shorttitle = {{{BiomedRAG}}},
  author = {Li, Mingchen and Kilicoglu, Halil and Xu, Hua and Zhang, Rui},
  date = {2025-02},
  journaltitle = {Journal of Biomedical Informatics},
  shortjournal = {Journal of Biomedical Informatics},
  volume = {162},
  pages = {104769},
  issn = {15320464},
  doi = {10.1016/j.jbi.2024.104769},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046424001874},
  urldate = {2025-09-30},
  langid = {english},
  file = {/home/zepi/Zotero/storage/Z8RFHBG5/Li et al. - 2025 - BiomedRAG A retrieval augmented large language model for biomedicine.pdf}
}

@article{liSurveyLLMbasedMultiagent2024,
  title = {A Survey on {{LLM-based}} Multi-Agent Systems: Workflow, Infrastructure, and Challenges},
  shorttitle = {A Survey on {{LLM-based}} Multi-Agent Systems},
  author = {Li, Xinyi and Wang, Sai and Zeng, Siqi and Wu, Yu and Yang, Yi},
  date = {2024-10-08},
  journaltitle = {Vicinagearth},
  shortjournal = {Vicinagearth},
  volume = {1},
  number = {1},
  pages = {9},
  issn = {3005-060X},
  doi = {10.1007/s44336-024-00009-2},
  url = {https://link.springer.com/10.1007/s44336-024-00009-2},
  urldate = {2025-09-30},
  abstract = {Abstract             The pursuit of more intelligent and credible autonomous systems, akin to human society, has been a long-standing endeavor for humans. Leveraging the exceptional reasoning and planning capabilities of large language models (LLMs), LLM-based agents have been proposed and have achieved remarkable success across a wide array of tasks. Notably, LLM-based multi-agent systems (MAS) are considered a promising pathway towards realizing general artificial intelligence that is equivalent to or surpasses human-level intelligence. In this paper, we present a comprehensive survey of these studies, offering a systematic review of LLM-based MAS. Adhering to the workflow of LLM-based multi-agent systems, we synthesize a general structure encompassing five key components: profile, perception, self-action, mutual interaction, and evolution. This unified framework encapsulates much of the previous work in the field. Furthermore, we illuminate the extensive applications of LLM-based MAS in two principal areas: problem-solving and world simulation. Finally, we discuss in detail several contemporary challenges and provide insights into potential future directions in this domain.},
  langid = {english},
  file = {/home/zepi/Zotero/storage/PUM8BKXF/Li et al. - 2024 - A survey on LLM-based multi-agent systems workflow, infrastructure, and challenges.pdf}
}

@article{liuAreLLMsGood2024,
  title = {Are {{LLMs}} Good at Structured Outputs? {{A}} Benchmark for Evaluating Structured Output Capabilities in {{LLMs}}},
  shorttitle = {Are {{LLMs}} Good at Structured Outputs?},
  author = {Liu, Yu and Li, Duantengchuan and Wang, Kaili and Xiong, Zhuoran and Shi, Fobo and Wang, Jian and Li, Bing and Hang, Bo},
  date = {2024-09},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {61},
  number = {5},
  pages = {103809},
  issn = {03064573},
  doi = {10.1016/j.ipm.2024.103809},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457324001687},
  urldate = {2025-09-30},
  langid = {english}
}

@article{liuTIPSTailoredInformation2025,
  title = {{{TIPS}}: {{Tailored Information Extraction}} in {{Public Security Using Domain-Enhanced Large Language Model}}},
  shorttitle = {{{TIPS}}},
  author = {Liu, Yue and Guo, Qinglang and Yang, Chunyao and Liao, Yong},
  date = {2025},
  journaltitle = {Computers, Materials \& Continua},
  shortjournal = {CMC},
  volume = {83},
  number = {2},
  pages = {2555--2572},
  issn = {1546-2226},
  doi = {10.32604/cmc.2025.060318},
  url = {https://www.techscience.com/cmc/v83n2/60528},
  urldate = {2025-09-30},
  langid = {english},
  file = {/home/zepi/Zotero/storage/M83KEFP9/Liu et al. - 2025 - TIPS Tailored Information Extraction in Public Security Using Domain-Enhanced Large Language Model.pdf}
}

@online{merilehtoPDFsStructuredData2024,
  title = {From {{PDFs}} to {{Structured Data}}: {{Utilizing LLM Analysis}} in {{Sports Database Management}} [{{Preprint}}]},
  shorttitle = {From {{PDFs}} to {{Structured Data}}},
  author = {Merilehto, Juhani},
  date = {2024-10-31},
  eprint = {2410.17619},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.17619},
  url = {http://arxiv.org/abs/2410.17619},
  urldate = {2025-09-16},
  abstract = {This study investigates the effectiveness of Large Language Models (LLMs) in processing semi-structured data from PDF documents into structured formats, specifically examining their application in updating the Finnish Sports Clubs Database. Through action research methodology, we developed and evaluated an AI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus models to process data from 72 sports federation membership reports. The system achieved a 90\% success rate in automated processing, successfully handling 65 of 72 files without errors and converting over 7,900 rows of data. While the initial development time was comparable to traditional manual processing (three months), the implemented system shows potential for reducing future processing time by approximately 90\%. Key challenges included handling multilingual content, processing multi-page datasets, and managing extraneous information. The findings suggest that while LLMs demonstrate significant potential for automating semi-structured data processing tasks, optimal results are achieved through a hybrid approach combining AI automation with selective human oversight. This research contributes to the growing body of literature on practical LLM applications in organizational data management and provides insights into the transformation of traditional data processing workflows.},
  pubstate = {prepublished},
  keywords = {Artificial Intelligence (cs.AI),Computational Engineering Finance and Science (cs.CE),Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,FOS: Computer and information sciences},
  file = {/home/zepi/Zotero/storage/FI2FJLUW/Merilehto - 2024 - From PDFs to Structured Data Utilizing LLM Analysis in Sports Database Management.pdf;/home/zepi/Zotero/storage/NUGUUYS5/2410.html}
}

@article{naveedComprehensiveOverviewLarge2025,
  title = {A {{Comprehensive Overview}} of {{Large Language Models}}},
  author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Akhtar, Naveed and Barnes, Nick and Mian, Ajmal},
  date = {2025-10-31},
  journaltitle = {ACM Transactions on Intelligent Systems and Technology},
  shortjournal = {ACM Trans. Intell. Syst. Technol.},
  volume = {16},
  number = {5},
  pages = {1--72},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3744746},
  url = {https://dl.acm.org/doi/10.1145/3744746},
  urldate = {2025-10-04},
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multimodal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only a systematic survey but also a quick, comprehensive reference for the researchers and practitioners to draw insights from extensive, informative summaries of the existing works to advance the LLM research.},
  langid = {english}
}

@inproceedings{paiSurveyOpenInformation2024,
  title = {A {{Survey}} on {{Open Information Extraction}} from {{Rule-based Model}} to {{Large Language Model}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Pai, Liu and Gao, Wenyang and Dong, Wenjie and Ai, Lin and Gong, Ziwei and Huang, Songfang and Zongsheng, Li and Hoque, Ehsan and Hirschberg, Julia and Zhang, Yue},
  date = {2024},
  pages = {9586--9608},
  publisher = {Association for Computational Linguistics},
  location = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.findings-emnlp.560},
  url = {https://aclanthology.org/2024.findings-emnlp.560},
  urldate = {2025-09-30},
  eventtitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  langid = {english},
  file = {/home/zepi/Zotero/storage/35ZM5DUB/Pai et al. - 2024 - A Survey on Open Information Extraction from Rule-based Model to Large Language Model.pdf}
}

@online{pengEmbeddingbasedRetrievalLLM2023,
  title = {Embedding-Based {{Retrieval}} with {{LLM}} for {{Effective Agriculture Information Extracting}} from {{Unstructured Data}} [{{Preprint}}]},
  author = {Peng, Ruoling and Liu, Kang and Yang, Po and Yuan, Zhipeng and Li, Shunbao},
  date = {2023-08-06},
  eprint = {2308.03107},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.03107},
  url = {http://arxiv.org/abs/2308.03107},
  urldate = {2025-09-16},
  abstract = {Pest identification is a crucial aspect of pest control in agriculture. However, most farmers are not capable of accurately identifying pests in the field, and there is a limited number of structured data sources available for rapid querying. In this work, we explored using domain-agnostic general pre-trained large language model(LLM) to extract structured data from agricultural documents with minimal or no human intervention. We propose a methodology that involves text retrieval and filtering using embedding-based retrieval, followed by LLM question-answering to automatically extract entities and attributes from the documents, and transform them into structured data. In comparison to existing methods, our approach achieves consistently better accuracy in the benchmark while maintaining efficiency.},
  pubstate = {prepublished},
  keywords = {Artificial Intelligence (cs.AI),Computer Science - Artificial Intelligence,FOS: Computer and information sciences},
  file = {/home/zepi/Zotero/storage/A9D7LCS8/Peng et al. - 2023 - Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructure.pdf;/home/zepi/Zotero/storage/YNI4JKYW/2308.html}
}

@online{shortenStructuredRAGJSONResponse2024,
  title = {{{StructuredRAG}}: {{JSON Response Formatting}} with {{Large Language Models}} [{{Preprint}}]},
  shorttitle = {{{StructuredRAG}}},
  author = {Shorten, Connor and Pierse, Charles and Smith, Thomas Benjamin and Cardenas, Erika and Sharma, Akanksha and Trengrove, John and family=Luijt, given=Bob, prefix=van, useprefix=true},
  date = {2024},
  doi = {10.48550/ARXIV.2408.11061},
  url = {https://arxiv.org/abs/2408.11061},
  urldate = {2025-09-30},
  abstract = {The ability of Large Language Models (LLMs) to generate structured outputs, such as JSON, is crucial for their use in Compound AI Systems. However, evaluating and improving this capability remains challenging. In this work, we introduce StructuredRAG, a benchmark of six tasks designed to assess LLMs' proficiency in following response format instructions. We evaluate two state-of-the-art LLMs, Gemini 1.5 Pro and Llama 3 8B-instruct with 4-bit quantization using two distinct prompting strategies. We introduce these prompting strategies as f-String and Follow the Format (FF) prompting. Across 24 experiments, we find an average success rate of 82.55\%. We further find a high variance in performance across tasks, models, and prompting strategies with success rates ranging from 0 to 100\%. We find that Llama 3 8B-instruct often performs competitively with Gemini 1.5 Pro. We observe that task complexity significantly influences performance, with tasks involving lists or composite object outputs proving more challenging. Our findings highlight the need for further research into improving the reliability and consistency of structured output generation in LLMs. We have open-sourced our experimental code and results at github.com/weaviate/structured-rag.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/home/zepi/Zotero/storage/ZFBG92ZT/Shorten et al. - 2024 - StructuredRAG JSON Response Formatting with Large Language Models [Preprint].pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and family=Kaiser, given=Ł, prefix=ukasz, useprefix=false and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2025-10-02},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/home/zepi/Zotero/storage/CE8AK4B4/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@online{wangSLOTStructuringOutput2025,
  title = {{{SLOT}}: {{Structuring}} the {{Output}} of {{Large Language Models}} [{{Preprint}}]},
  shorttitle = {{{SLOT}}},
  author = {Wang, Darren Yow-Bang and Shen, Zhengyuan and Mishra, Soumya Smruti and Xu, Zhichao and Teng, Yifei and Ding, Haibo},
  date = {2025},
  doi = {10.48550/ARXIV.2505.04016},
  url = {https://arxiv.org/abs/2505.04016},
  urldate = {2025-09-30},
  abstract = {Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5\%) and content similarity (94.0\%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/zepi/Zotero/storage/ZM22NQ2A/Wang et al. - 2025 - SLOT Structuring the Output of Large Language Models [Preprint].pdf}
}

@online{WasSindLarge2025,
  title = {Was sind Large Language Models (LLMs)? | IBM},
  shorttitle = {Was sind Large Language Models (LLMs)?},
  date = {2025-06-12T00:00:00.000},
  url = {https://www.ibm.com/de-de/think/topics/large-language-models},
  urldate = {2025-10-02},
  abstract = {Large Language Models sind KI-Systeme, die in der Lage sind, menschliche Sprache zu verstehen und zu generieren, indem sie große Mengen von Textdaten verarbeiten.},
  langid = {ngerman},
  organization = {Was sind große Sprachmodelle (Large Language Models, LLMs)?},
  file = {/home/zepi/Zotero/storage/WZQ7TP2B/large-language-models.html}
}

@online{wiestLLMAIxOpenSource2024,
  title = {{{LLM-AIx}}: {{An}} Open Source Pipeline for {{Information Extraction}} from Unstructured Medical Text Based on Privacy Preserving {{Large Language Models}} [{{Preprint}}]},
  shorttitle = {{{LLM-AIx}}},
  author = {Wiest, Isabella Catharina and Wolf, Fabian and Leßmann, Marie-Elisabeth and Van Treeck, Marko and Ferber, Dyke and Zhu, Jiefu and Boehme, Heiko and Bressem, Keno K. and Ulrich, Hannes and Ebert, Matthias P. and Kather, Jakob Nikolas},
  date = {2024-09-03},
  eprinttype = {Health Informatics},
  doi = {10.1101/2024.09.02.24312917},
  url = {http://medrxiv.org/lookup/doi/10.1101/2024.09.02.24312917},
  urldate = {2025-09-29},
  abstract = {Abstract           In clinical science and practice, text data, such as clinical letters or procedure reports, is stored in an unstructured way. This type of data is not a quantifiable resource for any kind of quantitative investigations and any manual review or structured information retrieval is time-consuming and costly. The capabilities of Large Language Models (LLMs) mark a paradigm shift in natural language processing and offer new possibilities for structured Information Extraction (IE) from medical free text. This protocol describes a workflow for LLM based information extraction (LLM-AIx), enabling extraction of predefined entities from unstructured text using privacy preserving LLMs. By converting unstructured clinical text into structured data, LLM-AIx addresses a critical barrier in clinical research and practice, where the efficient extraction of information is essential for improving clinical decision-making, enhancing patient outcomes, and facilitating large-scale data analysis.           The protocol consists of four main processing steps: 1) Problem definition and data preparation, 2) data preprocessing, 3) LLM-based IE and 4) output evaluation. LLM-AIx allows integration on local hospital hardware without the need of transferring any patient data to external servers. As example tasks, we applied LLM-AIx for the anonymization of fictitious clinical letters from patients with pulmonary embolism. Additionally, we extracted symptoms and laterality of the pulmonary embolism of these fictitious letters. We demonstrate troubleshooting for potential problems within the pipeline with an IE on a real-world dataset, 100 pathology reports from the Cancer Genome Atlas Program (TCGA), for TNM stage extraction. LLM-AIx can be executed without any programming knowledge via an easy-to-use interface and in no more than a few minutes or hours, depending on the LLM model selected.},
  langid = {english},
  pubstate = {prepublished},
  file = {/home/zepi/Zotero/storage/YDV636PD/Wiest et al. - 2024 - LLM-AIx An open source pipeline for Information Extraction from unstructured medical text based on.pdf}
}

@article{xingEntityExtractionKey2024,
  title = {Entity {{Extraction}} of {{Key Elements}} in 110 {{Police Reports Based}} on {{Large Language Models}}},
  author = {Xing, Xintao and Chen, Peng},
  date = {2024-09-03},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {14},
  number = {17},
  pages = {7819},
  issn = {2076-3417},
  doi = {10.3390/app14177819},
  url = {https://www.mdpi.com/2076-3417/14/17/7819},
  urldate = {2025-09-29},
  abstract = {With the rapid advancement of Internet technology and the increasing volume of police reports, relying solely on extensive human labor and traditional natural language processing methods for key element extraction has become impractical. Applying advanced technologies such as large language models to improve the effectiveness of police report extraction has become an inevitable trend in the field of police data analysis. This study addresses the characteristics of Chinese police reports and the need to extract key elements by employing large language models specific to the public security domain for entity extraction. Several lightweight (6/7b) open-source large language models were tested as base models. To enhance model performance, LoRA fine-tuning was employed, combined with data engineering approaches. A zero-shot data augmentation method based on ChatGPT and prompt engineering techniques tailored for police reports were proposed to further improve model performance. The key police report data from a certain city in 2019 were used as a sample for testing. Compared to the base models, prompt engineering improved the F1 score by approximately 3\%, while fine-tuning led to an increase of 10–50\% in the F1 score. After fine-tuning and comparing different base models, the Baichuan model demonstrated the best overall performance in extracting key elements from police reports. Using the data augmentation method to double the data size resulted in an additional 4\% increase in the F1 score, achieving optimal model performance. Compared to the fine-tuned universal information extraction (UIE) large language model, the police report entity extraction model constructed in this study improved the F1 score for each element by approximately 5\%, with a 42\% improvement in the F1 score for the “organization” element. Finally, ChatGPT was employed to align the extracted entities, resulting in a high-quality entity extraction outcome.},
  langid = {english},
  file = {/home/zepi/Zotero/storage/FKMFZB8U/Xing und Chen - 2024 - Entity Extraction of Key Elements in 110 Police Reports Based on Large Language Models.pdf}
}
